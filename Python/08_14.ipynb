{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✨ 다중 로지스틱 회귀 실습 ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 당뇨 데이터를 활용한 당뇨 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5000, W1 = -0.7133, W2 = -2.1019, W3 = 0.1283, W4 = -0.5249, W5 = -0.2082, W6 = -0.9658, W7 = -0.3619, W8 = 0.0189, b = 0.4189, cost = 0.4995\n",
      "step = 10000, W1 = -0.8332, W2 = -2.8493, W3 = 0.0250, W4 = -0.6205, W5 = -0.2910, W6 = -1.4362, W7 = -0.6406, W8 = -0.0420, b = 0.3123, cost = 0.4800\n",
      "step = 15000, W1 = -0.8625, W2 = -3.1866, W3 = 0.0331, W4 = -0.6444, W5 = -0.2972, W6 = -1.7349, W7 = -0.7983, W8 = -0.0409, b = 0.2743, cost = 0.4753\n",
      "step = 20000, W1 = -0.8749, W2 = -3.3604, W3 = 0.0724, W4 = -0.6459, W5 = -0.2970, W6 = -1.9434, W7 = -0.8841, W8 = -0.0411, b = 0.2498, cost = 0.4736\n",
      "step = 25000, W1 = -0.8820, W2 = -3.4553, W3 = 0.1204, W4 = -0.6383, W5 = -0.2991, W6 = -2.0968, W7 = -0.9307, W8 = -0.0464, b = 0.2301, cost = 0.4728\n",
      "step = 30000, W1 = -0.8866, W2 = -3.5088, W3 = 0.1687, W4 = -0.6267, W5 = -0.3034, W6 = -2.2134, W7 = -0.9562, W8 = -0.0544, b = 0.2134, cost = 0.4724\n",
      "step = 35000, W1 = -0.8898, W2 = -3.5395, W3 = 0.2138, W4 = -0.6138, W5 = -0.3089, W6 = -2.3042, W7 = -0.9702, W8 = -0.0633, b = 0.1991, cost = 0.4722\n",
      "step = 40000, W1 = -0.8921, W2 = -3.5573, W3 = 0.2542, W4 = -0.6007, W5 = -0.3149, W6 = -2.3761, W7 = -0.9779, W8 = -0.0720, b = 0.1869, cost = 0.4720\n",
      "step = 45000, W1 = -0.8938, W2 = -3.5679, W3 = 0.2897, W4 = -0.5884, W5 = -0.3209, W6 = -2.4339, W7 = -0.9821, W8 = -0.0800, b = 0.1765, cost = 0.4719\n",
      "step = 50000, W1 = -0.8951, W2 = -3.5743, W3 = 0.3203, W4 = -0.5770, W5 = -0.3265, W6 = -2.4807, W7 = -0.9844, W8 = -0.0870, b = 0.1677, cost = 0.4718\n",
      "\n",
      "Accuracy : 0.7681159420289855\n"
     ]
    }
   ],
   "source": [
    "Data_set = np.loadtxt(\"data-03-diabetes.csv\", delimiter = \",\")\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "x_data = Data_set[: , 0 : -1]\n",
    "y_data = Data_set[: , [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape = [None, 8])  # 개수가 정확하지 않을 때 : None 사용!\n",
    "Y = tf.placeholder(tf.float64, shape = [None, 1])\n",
    "\n",
    "# 기울기 a와 바이어스 b의 값을 임의로 정함\n",
    "W = tf.Variable(tf.random_uniform([8, 1], dtype = tf.float64), name = 'Weight')\n",
    "# [8, 1] 의미 : 들어오는 값은 8개, 나가는 값은 1개\n",
    "\n",
    "b = tf.Variable(tf.random_uniform([1], dtype = tf.float64), name = 'bias')\n",
    "\n",
    "# y 시그모이드 함수의 방정식을 세움\n",
    "y = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# 오차를 구하는 함수\n",
    "cost = -tf.reduce_mean(Y * tf.log(y) + (1 - Y) * tf.log(1 - y))\n",
    "\n",
    "# 학습률 값\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 오차를 최소로 하는 값 찾기\n",
    "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# 💥🚫⛔우리가 만든 모델의 정밀도를 측정하기 위해!💥🚫⛔\n",
    "predicted = tf.cast(y > 0.5, dtype = tf.float64)   ## 0.5 를 기준으로 0, 1을 반환해준다.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float64))  ## predicted 와 Y가 같은지, 다른지\n",
    "# => reduce_mean : 배열의 평균 (predicted(예측값)와 Y(실제값)를 비교했을 때 결과의 평균 = 정확도)\n",
    "\n",
    "\n",
    "# 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(50001):\n",
    "        W_, b_, cost_, _ = sess.run([W, b, cost, gradient_decent], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(\"step = %d, W1 = %.4f, W2 = %.4f, W3 = %.4f, W4 = %.4f, W5 = %.4f, W6 = %.4f, W7 = %.4f, W8 = %.4f, b = %.4f, cost = %.4f\" \n",
    "                 % (i + 1, W_[0], W_[1], W_[2], W_[3], W_[4], W_[5], W_[6], W_[7], b_, cost_))\n",
    "    \n",
    "    \n",
    "    # 추가 코드\n",
    "    #print(\"predicted = \", sess.run(predicted, feed_dict={X:x_data}))   # => y를 구하려면 X를 지정해줘야한다.\n",
    "\n",
    "    # 다른 값 테스트\n",
    "    #p_val, h_val = sess.run([predicted, y], feed_dict = {X:[[1, 5], [10, 5], [4, 5]]})\n",
    "    #print(\"check predicted =\", p_val)   # 연산된 값을 bool로 변환한 값\n",
    "    #print(\"check hypothesis =\", h_val)  # 계산값\n",
    "    \n",
    "    # 정확도 측정\n",
    "    #h, c, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    #print(\"\\nHypothesis :\", h, \"\\nCorrect (Y) :\", c, \"\\nAccuracy :\", a)        \n",
    "    \n",
    "    # 정확도 측정\n",
    "    _, _, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nAccuracy :\", a)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✨ 퍼셉트론 ✨\n",
    "🐶 신경망을 이루는 가장 중요한 기본 단위\n",
    "\n",
    "🐱 y = ax + b ( a는 기울기, b는 y 절편 ) 👉 y = Wx + b ( W는 가중치, b는 바이어스 )\n",
    "\n",
    "🦊 가중합 : W1x1+ W2x2 + b      \n",
    "🦊 활성화 함수  ex) 시그모이드\n",
    "\n",
    "🐰 XOR (exclusive OR) 문제를 해결하지 못 함! ([1,1] : 0, [0,0] : 0, [1,0] : 1, [0,1] : 1)             \n",
    "\n",
    "🤓 기존처럼 하나의 선이 아닌 두 개의 선을 그어 교집합을 찾자        \n",
    " x1 XOR x2 == NOT (x1 AND x2) AND (x1 OR x2) == (x1 NAND x2) AND (x1 OR x2)    \n",
    "\n",
    "🧐 Hidden Layer(은닉층)에는 두 개의 직선을 만들기 위해 두 개의 뉴런이 필요 => 이 둘 교차시키는 연산 필요 => ⭐다중 퍼셉트론⭐\n",
    "\n",
    "🐨오차 역전파는 경사 하강법의 확장 개념     \n",
    "신경망 내부의 가중치는 오차 역전파를 이용하여 수정하여 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1️⃣ single neural network AND 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 cost = 0.70660204 W = [[2.4511883 ]\n",
      " [0.39721212]] b = [-0.8031659]\n",
      "step = 100 cost = 0.3377292 W = [[1.8987201]\n",
      " [0.7005495]] b = [-2.2825046]\n",
      "step = 200 cost = 0.2782881 W = [[2.015758 ]\n",
      " [1.2689223]] b = [-2.786027]\n",
      "step = 300 cost = 0.23981419 W = [[2.1807175]\n",
      " [1.7022659]] b = [-3.1967237]\n",
      "step = 400 cost = 0.21204533 W = [[2.3562264]\n",
      " [2.0417883]] b = [-3.5589411]\n",
      "step = 500 cost = 0.19060236 W = [[2.5321186]\n",
      " [2.3203743]] b = [-3.8840754]\n",
      "step = 600 cost = 0.17331824 W = [[2.7033732]\n",
      " [2.5575235]] b = [-4.1793666]\n",
      "step = 700 cost = 0.1589841 W = [[2.8676171]\n",
      " [2.7650511]] b = [-4.4500313]\n",
      "step = 800 cost = 0.14685471 W = [[3.023933 ]\n",
      " [2.9504292]] b = [-4.699982]\n",
      "step = 900 cost = 0.1364345 W = [[3.1721797]\n",
      " [3.1185846]] b = [-4.9322295]\n",
      "step = 1000 cost = 0.127375 W = [[3.312608 ]\n",
      " [3.2729087]] b = [-5.14915]\n",
      "step = 1100 cost = 0.119420886 W = [[3.4456546]\n",
      " [3.4158213]] b = [-5.3526554]\n",
      "step = 1200 cost = 0.11237925 W = [[3.5718274]\n",
      " [3.5491076]] b = [-5.5443096]\n",
      "step = 1300 cost = 0.10610096 W = [[3.6916459]\n",
      " [3.6741312]] b = [-5.725408]\n",
      "step = 1400 cost = 0.10046807 W = [[3.8056078]\n",
      " [3.7919543]] b = [-5.8970494]\n",
      "step = 1500 cost = 0.09538638 W = [[3.914183 ]\n",
      " [3.9034286]] b = [-6.060157]\n",
      "step = 1600 cost = 0.09077913 W = [[4.0178   ]\n",
      " [4.0092483]] b = [-6.215527]\n",
      "step = 1700 cost = 0.086583376 W = [[4.1168504]\n",
      " [4.1099906]] b = [-6.3638525]\n",
      "step = 1800 cost = 0.08274685 W = [[4.211688]\n",
      " [4.206138]] b = [-6.5057297]\n",
      "step = 1900 cost = 0.07922585 W = [[4.302631]\n",
      " [4.298105]] b = [-6.6416845]\n",
      "step = 2000 cost = 0.07598341 W = [[4.389967 ]\n",
      " [4.3862476]] b = [-6.772182]\n",
      "step = 2100 cost = 0.072988175 W = [[4.473953 ]\n",
      " [4.4708786]] b = [-6.897634]\n",
      "step = 2200 cost = 0.0702132 W = [[4.5548267]\n",
      " [4.5522695]] b = [-7.018409]\n",
      "step = 2300 cost = 0.06763549 W = [[4.6328   ]\n",
      " [4.6306586]] b = [-7.134835]\n",
      "step = 2400 cost = 0.06523481 W = [[4.708065]\n",
      " [4.706262]] b = [-7.2472053]\n",
      "step = 2500 cost = 0.06299399 W = [[4.7807965]\n",
      " [4.779269 ]] b = [-7.355789]\n",
      "step = 2600 cost = 0.060897723 W = [[4.851152]\n",
      " [4.849851]] b = [-7.460826]\n",
      "step = 2700 cost = 0.058932684 W = [[4.9192753]\n",
      " [4.918164 ]] b = [-7.562534]\n",
      "step = 2800 cost = 0.05708702 W = [[4.9853005]\n",
      " [4.9843464]] b = [-7.661115]\n",
      "step = 2900 cost = 0.05535041 W = [[5.0493493]\n",
      " [5.0485272]] b = [-7.75675]\n",
      "step = 3000 cost = 0.05371357 W = [[5.1115327]\n",
      " [5.1108217]] b = [-7.849606]\n",
      "step = 3100 cost = 0.052168332 W = [[5.171953 ]\n",
      " [5.1713357]] b = [-7.9398365]\n",
      "step = 3200 cost = 0.050707236 W = [[5.230704]\n",
      " [5.230167]] b = [-8.027581]\n",
      "step = 3300 cost = 0.049323674 W = [[5.287874 ]\n",
      " [5.2874055]] b = [-8.112971]\n",
      "step = 3400 cost = 0.04801185 W = [[5.343545]\n",
      " [5.343133]] b = [-8.196127]\n",
      "step = 3500 cost = 0.0467662 W = [[5.3977895]\n",
      " [5.3974266]] b = [-8.277162]\n",
      "step = 3600 cost = 0.04558204 W = [[5.4506783]\n",
      " [5.450358 ]] b = [-8.356176]\n",
      "step = 3700 cost = 0.044454984 W = [[5.5022745]\n",
      " [5.5019913]] b = [-8.433269]\n",
      "step = 3800 cost = 0.043381058 W = [[5.5526414]\n",
      " [5.5523863]] b = [-8.508526]\n",
      "step = 3900 cost = 0.042356588 W = [[5.6018305]\n",
      " [5.601604 ]] b = [-8.582033]\n",
      "step = 4000 cost = 0.04137838 W = [[5.6498966]\n",
      " [5.6496944]] b = [-8.653868]\n",
      "step = 4100 cost = 0.0404433 W = [[5.6968884]\n",
      " [5.6967077]] b = [-8.724101]\n",
      "step = 4200 cost = 0.03954865 W = [[5.7428527]\n",
      " [5.7426896]] b = [-8.792805]\n",
      "step = 4300 cost = 0.0386919 W = [[5.7878327]\n",
      " [5.787684 ]] b = [-8.860041]\n",
      "step = 4400 cost = 0.03787069 W = [[5.8318667]\n",
      " [5.831732 ]] b = [-8.925867]\n",
      "step = 4500 cost = 0.037082918 W = [[5.8749943]\n",
      " [5.874872 ]] b = [-8.990344]\n",
      "step = 4600 cost = 0.036326565 W = [[5.91725]\n",
      " [5.91714]] b = [-9.053524]\n",
      "step = 4700 cost = 0.03559988 W = [[5.95867  ]\n",
      " [5.9585705]] b = [-9.115456]\n",
      "step = 4800 cost = 0.034901068 W = [[5.999284 ]\n",
      " [5.9991937]] b = [-9.1761875]\n",
      "step = 4900 cost = 0.034228697 W = [[6.039123]\n",
      " [6.039041]] b = [-9.2357645]\n",
      "step = 5000 cost = 0.03358121 W = [[6.078216]\n",
      " [6.078142]] b = [-9.29423]\n",
      "step = 5100 cost = 0.032957297 W = [[6.11659  ]\n",
      " [6.1165223]] b = [-9.351622]\n",
      "step = 5200 cost = 0.032355692 W = [[6.15427 ]\n",
      " [6.154209]] b = [-9.407978]\n",
      "step = 5300 cost = 0.03177531 W = [[6.1912794]\n",
      " [6.1912236]] b = [-9.463337]\n",
      "step = 5400 cost = 0.031214967 W = [[6.227643]\n",
      " [6.227591]] b = [-9.51773]\n",
      "step = 5500 cost = 0.030673733 W = [[6.263381 ]\n",
      " [6.2633333]] b = [-9.571191]\n",
      "step = 5600 cost = 0.030150615 W = [[6.298515 ]\n",
      " [6.2984705]] b = [-9.623751]\n",
      "step = 5700 cost = 0.029644724 W = [[6.3330646]\n",
      " [6.333024 ]] b = [-9.675439]\n",
      "step = 5800 cost = 0.029155215 W = [[6.367048 ]\n",
      " [6.3670106]] b = [-9.726282]\n",
      "step = 5900 cost = 0.028681394 W = [[6.400484 ]\n",
      " [6.4004498]] b = [-9.776309]\n",
      "step = 6000 cost = 0.028222457 W = [[6.4333897]\n",
      " [6.4333572]] b = [-9.825544]\n",
      "step = 6100 cost = 0.027777687 W = [[6.4657807]\n",
      " [6.4657507]] b = [-9.874012]\n",
      "step = 6200 cost = 0.027346509 W = [[6.497673]\n",
      " [6.497646]] b = [-9.921737]\n",
      "step = 6300 cost = 0.02692834 W = [[6.5290823]\n",
      " [6.5290556]] b = [-9.968736]\n",
      "step = 6400 cost = 0.02652252 W = [[6.560021]\n",
      " [6.559996]] b = [-10.015036]\n",
      "step = 6500 cost = 0.026128605 W = [[6.590504 ]\n",
      " [6.5904813]] b = [-10.060655]\n",
      "step = 6600 cost = 0.025746018 W = [[6.6205435]\n",
      " [6.620523 ]] b = [-10.105613]\n",
      "step = 6700 cost = 0.025374305 W = [[6.650153]\n",
      " [6.650134]] b = [-10.149929]\n",
      "step = 6800 cost = 0.025012989 W = [[6.6793437]\n",
      " [6.679326 ]] b = [-10.193619]\n",
      "step = 6900 cost = 0.0246617 W = [[6.7081275]\n",
      " [6.7081122]] b = [-10.236702]\n",
      "step = 7000 cost = 0.024319945 W = [[6.7365155]\n",
      " [6.7365007]] b = [-10.279194]\n",
      "step = 7100 cost = 0.023987481 W = [[6.764518]\n",
      " [6.764505]] b = [-10.321109]\n",
      "step = 7200 cost = 0.023663776 W = [[6.7921457]\n",
      " [6.7921343]] b = [-10.362463]\n",
      "step = 7300 cost = 0.023348628 W = [[6.8194065]\n",
      " [6.819397 ]] b = [-10.403272]\n",
      "step = 7400 cost = 0.023041613 W = [[6.8463135]\n",
      " [6.846304 ]] b = [-10.44355]\n",
      "step = 7500 cost = 0.02274246 W = [[6.8728733]\n",
      " [6.872864 ]] b = [-10.483309]\n",
      "step = 7600 cost = 0.022450881 W = [[6.899094]\n",
      " [6.899085]] b = [-10.522563]\n",
      "step = 7700 cost = 0.022166576 W = [[6.9249854]\n",
      " [6.9249763]] b = [-10.561324]\n",
      "step = 7800 cost = 0.021889277 W = [[6.9505544]\n",
      " [6.9505453]] b = [-10.599605]\n",
      "step = 7900 cost = 0.021618746 W = [[6.97581 ]\n",
      " [6.975802]] b = [-10.637417]\n",
      "step = 8000 cost = 0.021354757 W = [[7.000758 ]\n",
      " [7.0007505]] b = [-10.674772]\n",
      "step = 8100 cost = 0.021097044 W = [[7.025408 ]\n",
      " [7.0254006]] b = [-10.711679]\n",
      "step = 8200 cost = 0.020845372 W = [[7.0497656]\n",
      " [7.0497584]] b = [-10.748148]\n",
      "step = 8300 cost = 0.020599551 W = [[7.0738373]\n",
      " [7.0738316]] b = [-10.784192]\n",
      "step = 8400 cost = 0.02035939 W = [[7.097631 ]\n",
      " [7.0976253]] b = [-10.819821]\n",
      "step = 8500 cost = 0.020124745 W = [[7.121153]\n",
      " [7.121147]] b = [-10.85504]\n",
      "step = 8600 cost = 0.019895323 W = [[7.1444063]\n",
      " [7.1444006]] b = [-10.88986]\n",
      "step = 8700 cost = 0.01967102 W = [[7.1674013]\n",
      " [7.1673956]] b = [-10.924292]\n",
      "step = 8800 cost = 0.019451685 W = [[7.1901393]\n",
      " [7.1901336]] b = [-10.958343]\n",
      "step = 8900 cost = 0.019237146 W = [[7.212629]\n",
      " [7.212623]] b = [-10.9920225]\n",
      "step = 9000 cost = 0.019027162 W = [[7.2348742]\n",
      " [7.2348685]] b = [-11.025337]\n",
      "step = 9100 cost = 0.01882172 W = [[7.2568808]\n",
      " [7.2568755]] b = [-11.058292]\n",
      "step = 9200 cost = 0.018620603 W = [[7.278654]\n",
      " [7.278649]] b = [-11.0908985]\n",
      "step = 9300 cost = 0.01842365 W = [[7.300199 ]\n",
      " [7.3001943]] b = [-11.123166]\n",
      "step = 9400 cost = 0.018230814 W = [[7.321518 ]\n",
      " [7.3215137]] b = [-11.155093]\n",
      "step = 9500 cost = 0.018041896 W = [[7.342618 ]\n",
      " [7.3426137]] b = [-11.186695]\n",
      "step = 9600 cost = 0.017856842 W = [[7.363503 ]\n",
      " [7.3634987]] b = [-11.217973]\n",
      "step = 9700 cost = 0.017675499 W = [[7.384177]\n",
      " [7.384173]] b = [-11.248935]\n",
      "step = 9800 cost = 0.017497743 W = [[7.404643 ]\n",
      " [7.4046392]] b = [-11.27959]\n",
      "step = 9900 cost = 0.017323509 W = [[7.4249077]\n",
      " [7.424904 ]] b = [-11.309942]\n",
      "step = 10000 cost = 0.017152626 W = [[7.4449735]\n",
      " [7.4449697]] b = [-11.339997]\n",
      "\\hypothesis : [[1.1831522e-05]\n",
      " [1.9937217e-02]\n",
      " [1.9937277e-02]\n",
      " [9.7207594e-01]] \n",
      "Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [0], [0], [1]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'y-input')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#활성화 함수(hypothesis : 가설)\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# 예측값 결과 계산 및 정밀도 계산\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W =\", sess.run(W), \"b =\", sess.run(b))\n",
    "            \n",
    "    # Accuracy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis :\", h, \"\\nCorrect :\", c, \"\\nAccuracy :\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2️⃣ single neural network NAND 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 cost = 0.68160236 W = [[ 1.0457289]\n",
      " [-0.6151815]] b = [1.6071373]\n",
      "step = 100 cost = 0.41311142 W = [[-0.2684953]\n",
      " [-1.3226477]] b = [1.7036629]\n",
      "step = 200 cost = 0.3289925 W = [[-0.94491595]\n",
      " [-1.5826888 ]] b = [2.2606018]\n",
      "step = 300 cost = 0.27684397 W = [[-1.4247683]\n",
      " [-1.8200564]] b = [2.7436142]\n",
      "step = 400 cost = 0.2405568 W = [[-1.7951871]\n",
      " [-2.0471635]] b = [3.1601415]\n",
      "step = 500 cost = 0.21336022 W = [[-2.096842 ]\n",
      " [-2.2618775]] b = [3.5270488]\n",
      "step = 600 cost = 0.19196668 W = [[-2.3523986]\n",
      " [-2.4632437]] b = [3.8556275]\n",
      "step = 700 cost = 0.17457573 W = [[-2.5752487]\n",
      " [-2.6514344]] b = [4.153577]\n",
      "step = 800 cost = 0.16010071 W = [[-2.7737365]\n",
      " [-2.8272102]] b = [4.426387]\n",
      "step = 900 cost = 0.14783579 W = [[-2.9533134]\n",
      " [-2.9915717]] b = [4.6781197]\n",
      "step = 1000 cost = 0.13729614 W = [[-3.1177135]\n",
      " [-3.145565 ]] b = [4.911886]\n",
      "step = 1100 cost = 0.12813455 W = [[-3.2695944]\n",
      " [-3.2901967]] b = [5.1301184]\n",
      "step = 1200 cost = 0.12009388 W = [[-3.4109237]\n",
      " [-3.426386 ]] b = [5.334774]\n",
      "step = 1300 cost = 0.11297895 W = [[-3.543198 ]\n",
      " [-3.5549595]] b = [5.5274453]\n",
      "step = 1400 cost = 0.10663821 W = [[-3.6675894]\n",
      " [-3.6766477]] b = [5.7094517]\n",
      "step = 1500 cost = 0.10095194 W = [[-3.7850416]\n",
      " [-3.7920961]] b = [5.881908]\n",
      "step = 1600 cost = 0.09582424 W = [[-3.8963215]\n",
      " [-3.9018729]] b = [6.045753]\n",
      "step = 1700 cost = 0.09117718 W = [[-4.002067]\n",
      " [-4.006477]] b = [6.2017927]\n",
      "step = 1800 cost = 0.086946696 W = [[-4.102815 ]\n",
      " [-4.1063523]] b = [6.3507266]\n",
      "step = 1900 cost = 0.083079755 W = [[-4.1990275]\n",
      " [-4.2018876]] b = [6.493162]\n",
      "step = 2000 cost = 0.07953198 W = [[-4.291097 ]\n",
      " [-4.2934275]] b = [6.6296315]\n",
      "step = 2100 cost = 0.07626576 W = [[-4.3793683]\n",
      " [-4.3812823]] b = [6.7606044]\n",
      "step = 2200 cost = 0.07324938 W = [[-4.4641423]\n",
      " [-4.4657254]] b = [6.886496]\n",
      "step = 2300 cost = 0.07045549 W = [[-4.545684 ]\n",
      " [-4.5470014]] b = [7.00768]\n",
      "step = 2400 cost = 0.06786079 W = [[-4.6242313]\n",
      " [-4.625333 ]] b = [7.1244855]\n",
      "step = 2500 cost = 0.06544495 W = [[-4.699992]\n",
      " [-4.70092 ]] b = [7.2372117]\n",
      "step = 2600 cost = 0.06319031 W = [[-4.773157]\n",
      " [-4.773941]] b = [7.346128]\n",
      "step = 2700 cost = 0.061081532 W = [[-4.8438993]\n",
      " [-4.8445597]] b = [7.4514766]\n",
      "step = 2800 cost = 0.05910512 W = [[-4.912363]\n",
      " [-4.912927]] b = [7.553476]\n",
      "step = 2900 cost = 0.057249106 W = [[-4.978693]\n",
      " [-4.979177]] b = [7.652332]\n",
      "step = 3000 cost = 0.05550307 W = [[-5.0430174]\n",
      " [-5.0434327]] b = [7.7482257]\n",
      "step = 3100 cost = 0.053857602 W = [[-5.105449]\n",
      " [-5.105808]] b = [7.841326]\n",
      "step = 3200 cost = 0.052304365 W = [[-5.1660957]\n",
      " [-5.1664076]] b = [7.9317875]\n",
      "step = 3300 cost = 0.050835926 W = [[-5.225055 ]\n",
      " [-5.2253265]] b = [8.019752]\n",
      "step = 3400 cost = 0.049445648 W = [[-5.2824173]\n",
      " [-5.282654 ]] b = [8.1053505]\n",
      "step = 3500 cost = 0.04812749 W = [[-5.3382635]\n",
      " [-5.3384714]] b = [8.1887045]\n",
      "step = 3600 cost = 0.046876106 W = [[-5.3926716]\n",
      " [-5.3928547]] b = [8.269926]\n",
      "step = 3700 cost = 0.04568662 W = [[-5.445712 ]\n",
      " [-5.4458737]] b = [8.349119]\n",
      "step = 3800 cost = 0.044554554 W = [[-5.497451 ]\n",
      " [-5.4975934]] b = [8.426381]\n",
      "step = 3900 cost = 0.04347598 W = [[-5.5479507]\n",
      " [-5.5480733]] b = [8.5018015]\n",
      "step = 4000 cost = 0.042447206 W = [[-5.5972652]\n",
      " [-5.5973735]] b = [8.575463]\n",
      "step = 4100 cost = 0.041464955 W = [[-5.6454487]\n",
      " [-5.645545 ]] b = [8.647445]\n",
      "step = 4200 cost = 0.040526096 W = [[-5.6925516]\n",
      " [-5.6926374]] b = [8.717819]\n",
      "step = 4300 cost = 0.039627854 W = [[-5.7386203]\n",
      " [-5.738697 ]] b = [8.786658]\n",
      "step = 4400 cost = 0.03876779 W = [[-5.7836986]\n",
      " [-5.7837663]] b = [8.854024]\n",
      "step = 4500 cost = 0.03794348 W = [[-5.827827 ]\n",
      " [-5.8278875]] b = [8.919975]\n",
      "step = 4600 cost = 0.037152775 W = [[-5.871044 ]\n",
      " [-5.8710985]] b = [8.984572]\n",
      "step = 4700 cost = 0.036393687 W = [[-5.913386]\n",
      " [-5.913436]] b = [9.047867]\n",
      "step = 4800 cost = 0.035664342 W = [[-5.954888 ]\n",
      " [-5.9549327]] b = [9.10991]\n",
      "step = 4900 cost = 0.03496314 W = [[-5.9955792]\n",
      " [-5.9956226]] b = [9.170748]\n",
      "step = 5000 cost = 0.034288384 W = [[-6.035494]\n",
      " [-6.035533]] b = [9.230429]\n",
      "step = 5100 cost = 0.033638738 W = [[-6.074659]\n",
      " [-6.074694]] b = [9.288993]\n",
      "step = 5200 cost = 0.033012707 W = [[-6.113101 ]\n",
      " [-6.1131334]] b = [9.34648]\n",
      "step = 5300 cost = 0.0324092 W = [[-6.1508474]\n",
      " [-6.150876 ]] b = [9.402928]\n",
      "step = 5400 cost = 0.03182689 W = [[-6.1879206]\n",
      " [-6.1879473]] b = [9.458375]\n",
      "step = 5500 cost = 0.031264827 W = [[-6.2243447]\n",
      " [-6.224369 ]] b = [9.512854]\n",
      "step = 5600 cost = 0.030721908 W = [[-6.2601414]\n",
      " [-6.260164 ]] b = [9.566398]\n",
      "step = 5700 cost = 0.03019714 W = [[-6.295332 ]\n",
      " [-6.2953544]] b = [9.619038]\n",
      "step = 5800 cost = 0.029689776 W = [[-6.3299356]\n",
      " [-6.329956 ]] b = [9.670803]\n",
      "step = 5900 cost = 0.02919883 W = [[-6.3639717]\n",
      " [-6.363991 ]] b = [9.721723]\n",
      "step = 6000 cost = 0.028723598 W = [[-6.3974586]\n",
      " [-6.3974767]] b = [9.771822]\n",
      "step = 6100 cost = 0.028263316 W = [[-6.430413]\n",
      " [-6.43043 ]] b = [9.821128]\n",
      "step = 6200 cost = 0.027817346 W = [[-6.4628515]\n",
      " [-6.4628673]] b = [9.869663]\n",
      "step = 6300 cost = 0.027384983 W = [[-6.49479 ]\n",
      " [-6.494806]] b = [9.917454]\n",
      "step = 6400 cost = 0.026965614 W = [[-6.5262427]\n",
      " [-6.526258 ]] b = [9.964518]\n",
      "step = 6500 cost = 0.026558697 W = [[-6.5572243]\n",
      " [-6.5572386]] b = [10.010881]\n",
      "step = 6600 cost = 0.026163757 W = [[-6.5877495]\n",
      " [-6.587763 ]] b = [10.0565605]\n",
      "step = 6700 cost = 0.025780147 W = [[-6.6178293]\n",
      " [-6.6178417]] b = [10.101577]\n",
      "step = 6800 cost = 0.025407428 W = [[-6.6474786]\n",
      " [-6.6474905]] b = [10.14595]\n",
      "step = 6900 cost = 0.025045218 W = [[-6.676708]\n",
      " [-6.676719]] b = [10.189698]\n",
      "step = 7000 cost = 0.024693035 W = [[-6.7055297]\n",
      " [-6.7055407]] b = [10.232833]\n",
      "step = 7100 cost = 0.024350481 W = [[-6.733954]\n",
      " [-6.733965]] b = [10.275376]\n",
      "step = 7200 cost = 0.024017155 W = [[-6.761992]\n",
      " [-6.762002]] b = [10.317344]\n",
      "step = 7300 cost = 0.023692697 W = [[-6.7896547]\n",
      " [-6.789663 ]] b = [10.358749]\n",
      "step = 7400 cost = 0.023376781 W = [[-6.816951]\n",
      " [-6.816958]] b = [10.399609]\n",
      "step = 7500 cost = 0.023069002 W = [[-6.8438888]\n",
      " [-6.843896 ]] b = [10.439933]\n",
      "step = 7600 cost = 0.022769175 W = [[-6.8704796]\n",
      " [-6.8704863]] b = [10.479739]\n",
      "step = 7700 cost = 0.022476899 W = [[-6.896731 ]\n",
      " [-6.8967376]] b = [10.519039]\n",
      "step = 7800 cost = 0.022191968 W = [[-6.922652 ]\n",
      " [-6.9226584]] b = [10.557844]\n",
      "step = 7900 cost = 0.021914046 W = [[-6.948251 ]\n",
      " [-6.9482574]] b = [10.596168]\n",
      "step = 8000 cost = 0.02164289 W = [[-6.9735336]\n",
      " [-6.97354  ]] b = [10.634023]\n",
      "step = 8100 cost = 0.021378323 W = [[-6.998511 ]\n",
      " [-6.9985166]] b = [10.671416]\n",
      "step = 8200 cost = 0.021120008 W = [[-7.0231876]\n",
      " [-7.0231934]] b = [10.708363]\n",
      "step = 8300 cost = 0.020867838 W = [[-7.0475717]\n",
      " [-7.047577 ]] b = [10.744873]\n",
      "step = 8400 cost = 0.020621516 W = [[-7.0716705]\n",
      " [-7.071676 ]] b = [10.780954]\n",
      "step = 8500 cost = 0.020380871 W = [[-7.095489 ]\n",
      " [-7.0954943]] b = [10.81662]\n",
      "step = 8600 cost = 0.0201457 W = [[-7.119034]\n",
      " [-7.119039]] b = [10.851877]\n",
      "step = 8700 cost = 0.019915812 W = [[-7.1423125]\n",
      " [-7.142318 ]] b = [10.886735]\n",
      "step = 8800 cost = 0.01969106 W = [[-7.165329]\n",
      " [-7.165334]] b = [10.921201]\n",
      "step = 8900 cost = 0.01947129 W = [[-7.1880913]\n",
      " [-7.188096 ]] b = [10.955286]\n",
      "step = 9000 cost = 0.019256284 W = [[-7.2106037]\n",
      " [-7.2106085]] b = [10.988997]\n",
      "step = 9100 cost = 0.01904593 W = [[-7.2328715]\n",
      " [-7.232876 ]] b = [11.022343]\n",
      "step = 9200 cost = 0.018840056 W = [[-7.2549005]\n",
      " [-7.254904 ]] b = [11.05533]\n",
      "step = 9300 cost = 0.01863858 W = [[-7.276694]\n",
      " [-7.276697]] b = [11.087971]\n",
      "step = 9400 cost = 0.018441286 W = [[-7.2982583]\n",
      " [-7.298261 ]] b = [11.120264]\n",
      "step = 9500 cost = 0.018248033 W = [[-7.3195987]\n",
      " [-7.3196015]] b = [11.152225]\n",
      "step = 9600 cost = 0.018058788 W = [[-7.3407187]\n",
      " [-7.3407216]] b = [11.183853]\n",
      "step = 9700 cost = 0.01787341 W = [[-7.3616223]\n",
      " [-7.361625 ]] b = [11.215163]\n",
      "step = 9800 cost = 0.017691696 W = [[-7.382315]\n",
      " [-7.382318]] b = [11.246154]\n",
      "step = 9900 cost = 0.017513616 W = [[-7.402802 ]\n",
      " [-7.4028044]] b = [11.2768345]\n",
      "step = 10000 cost = 0.017339103 W = [[-7.423084]\n",
      " [-7.423086]] b = [11.307213]\n",
      "\\hypothesis : [[0.99998766]\n",
      " [0.9798486 ]\n",
      " [0.97984874]\n",
      " [0.0282239 ]] \n",
      "Correct : [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'y-input')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#활성화 함수(hypothesis : 가설)\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# 예측값 결과 계산 및 정밀도 계산\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W =\", sess.run(W), \"b =\", sess.run(b))\n",
    "            \n",
    "    # Accuracy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis :\", h, \"\\nCorrect :\", c, \"\\nAccuracy :\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3️⃣ single neural network OR 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 cost = 0.9201837 W = [[-0.5642119]\n",
      " [ 0.7443477]] b = [-0.76706487]\n",
      "step = 100 cost = 0.3114297 W = [[0.95614994]\n",
      " [1.7542955 ]] b = [0.08526676]\n",
      "step = 200 cost = 0.24459726 W = [[1.6062078]\n",
      " [2.1677344]] b = [-0.1765819]\n",
      "step = 300 cost = 0.20122166 W = [[2.097698]\n",
      " [2.50998 ]] b = [-0.4489475]\n",
      "step = 400 cost = 0.1703693 W = [[2.502319]\n",
      " [2.813876]] b = [-0.67706275]\n",
      "step = 500 cost = 0.14736432 W = [[2.8464632]\n",
      " [3.087842 ]] b = [-0.86770064]\n",
      "step = 600 cost = 0.12958482 W = [[3.1453795]\n",
      " [3.336548 ]] b = [-1.03041]\n",
      "step = 700 cost = 0.11545509 W = [[3.4092102]\n",
      " [3.5635688]] b = [-1.1721374]\n",
      "step = 800 cost = 0.10397346 W = [[3.645096 ]\n",
      " [3.7718577]] b = [-1.2976443]\n",
      "step = 900 cost = 0.09447248 W = [[3.8582392]\n",
      " [3.9638906]] b = [-1.4102535]\n",
      "step = 1000 cost = 0.08649027 W = [[4.052537 ]\n",
      " [4.1417418]] b = [-1.5123588]\n",
      "step = 1100 cost = 0.07969707 W = [[4.230974]\n",
      " [4.307163]] b = [-1.6057395]\n",
      "step = 1200 cost = 0.0738512 W = [[4.395892]\n",
      " [4.461624]] b = [-1.6917537]\n",
      "step = 1300 cost = 0.06877154 W = [[4.5491514]\n",
      " [4.6063704]] b = [-1.7714636]\n",
      "step = 1400 cost = 0.064319864 W = [[4.692256]\n",
      " [4.74247 ]] b = [-1.8457161]\n",
      "step = 1500 cost = 0.06038892 W = [[4.8264427]\n",
      " [4.870827 ]] b = [-1.9151986]\n",
      "step = 1600 cost = 0.056894198 W = [[4.952737]\n",
      " [4.992222]] b = [-1.9804755]\n",
      "step = 1700 cost = 0.05376844 W = [[5.0719905]\n",
      " [5.107328 ]] b = [-2.042018]\n",
      "step = 1800 cost = 0.050957296 W = [[5.184935 ]\n",
      " [5.2167277]] b = [-2.1002207]\n",
      "step = 1900 cost = 0.04841636 W = [[5.29219 ]\n",
      " [5.320939]] b = [-2.15542]\n",
      "step = 2000 cost = 0.04610932 W = [[5.394292]\n",
      " [5.420402]] b = [-2.2079053]\n",
      "step = 2100 cost = 0.04400578 W = [[5.4917006]\n",
      " [5.515513 ]] b = [-2.2579238]\n",
      "step = 2200 cost = 0.04208045 W = [[5.5848236]\n",
      " [5.606621 ]] b = [-2.3056943]\n",
      "step = 2300 cost = 0.040311962 W = [[5.6740136]\n",
      " [5.694036 ]] b = [-2.3514054]\n",
      "step = 2400 cost = 0.038682293 W = [[5.7595835]\n",
      " [5.778036 ]] b = [-2.3952234]\n",
      "step = 2500 cost = 0.037175898 W = [[5.8418097]\n",
      " [5.8588676]] b = [-2.4372964]\n",
      "step = 2600 cost = 0.03577953 W = [[5.92094  ]\n",
      " [5.9367514]] b = [-2.4777558]\n",
      "step = 2700 cost = 0.03448182 W = [[5.9971933]\n",
      " [6.0118895]] b = [-2.5167165]\n",
      "step = 2800 cost = 0.033272706 W = [[6.07077  ]\n",
      " [6.0844617]] b = [-2.5542846]\n",
      "step = 2900 cost = 0.032143656 W = [[6.141848 ]\n",
      " [6.1546316]] b = [-2.5905547]\n",
      "step = 3000 cost = 0.031087052 W = [[6.210587]\n",
      " [6.22255 ]] b = [-2.6256099]\n",
      "step = 3100 cost = 0.030096207 W = [[6.2771344]\n",
      " [6.288353 ]] b = [-2.6595302]\n",
      "step = 3200 cost = 0.029165275 W = [[6.341624 ]\n",
      " [6.3521624]] b = [-2.6923842]\n",
      "step = 3300 cost = 0.028289078 W = [[6.404176 ]\n",
      " [6.4140954]] b = [-2.724236]\n",
      "step = 3400 cost = 0.027462875 W = [[6.464903 ]\n",
      " [6.4742546]] b = [-2.7551432]\n",
      "step = 3500 cost = 0.026682677 W = [[6.5239058]\n",
      " [6.5327363]] b = [-2.78516]\n",
      "step = 3600 cost = 0.025944786 W = [[6.5812783]\n",
      " [6.5896297]] b = [-2.8143358]\n",
      "step = 3700 cost = 0.025245853 W = [[6.637108 ]\n",
      " [6.6450176]] b = [-2.8427155]\n",
      "step = 3800 cost = 0.024582861 W = [[6.6914735]\n",
      " [6.6989746]] b = [-2.8703387]\n",
      "step = 3900 cost = 0.023953252 W = [[6.744448]\n",
      " [6.751572]] b = [-2.8972456]\n",
      "step = 4000 cost = 0.023354491 W = [[6.7961006]\n",
      " [6.8028746]] b = [-2.9234724]\n",
      "step = 4100 cost = 0.022784479 W = [[6.846494 ]\n",
      " [6.8529444]] b = [-2.9490514]\n",
      "step = 4200 cost = 0.022241168 W = [[6.8956885]\n",
      " [6.9018354]] b = [-2.9740145]\n",
      "step = 4300 cost = 0.021722686 W = [[6.9437385]\n",
      " [6.9496036]] b = [-2.9983883]\n",
      "step = 4400 cost = 0.021227527 W = [[6.990695]\n",
      " [6.996298]] b = [-3.0222003]\n",
      "step = 4500 cost = 0.020754065 W = [[7.036607]\n",
      " [7.041963]] b = [-3.0454764]\n",
      "step = 4600 cost = 0.020300912 W = [[7.081518 ]\n",
      " [7.0866423]] b = [-3.0682387]\n",
      "step = 4700 cost = 0.019866906 W = [[7.125471]\n",
      " [7.13038 ]] b = [-3.09051]\n",
      "step = 4800 cost = 0.01945072 W = [[7.1685057]\n",
      " [7.173211 ]] b = [-3.1123097]\n",
      "step = 4900 cost = 0.01905141 W = [[7.2106586]\n",
      " [7.2151737]] b = [-3.133658]\n",
      "step = 5000 cost = 0.018667912 W = [[7.2519655]\n",
      " [7.256301 ]] b = [-3.1545725]\n",
      "step = 5100 cost = 0.018299397 W = [[7.2924576]\n",
      " [7.296626 ]] b = [-3.1750703]\n",
      "step = 5200 cost = 0.017944887 W = [[7.3321676]\n",
      " [7.3361764]] b = [-3.1951673]\n",
      "step = 5300 cost = 0.017603667 W = [[7.3711247]\n",
      " [7.3749833]] b = [-3.2148795]\n",
      "step = 5400 cost = 0.017275088 W = [[7.409356]\n",
      " [7.413073]] b = [-3.234221]\n",
      "step = 5500 cost = 0.016958328 W = [[7.446888]\n",
      " [7.45047 ]] b = [-3.2532043]\n",
      "step = 5600 cost = 0.016652869 W = [[7.4837456]\n",
      " [7.4872007]] b = [-3.2718437]\n",
      "step = 5700 cost = 0.016358016 W = [[7.5199523]\n",
      " [7.523286 ]] b = [-3.29015]\n",
      "step = 5800 cost = 0.016073313 W = [[7.5555305]\n",
      " [7.5587497]] b = [-3.308135]\n",
      "step = 5900 cost = 0.015798269 W = [[7.5905004]\n",
      " [7.593612 ]] b = [-3.3258104]\n",
      "step = 6000 cost = 0.015532368 W = [[7.624884 ]\n",
      " [7.6278925]] b = [-3.3431861]\n",
      "step = 6100 cost = 0.015275156 W = [[7.6587005]\n",
      " [7.6616096]] b = [-3.360272]\n",
      "step = 6200 cost = 0.015026238 W = [[7.6919665]\n",
      " [7.694781 ]] b = [-3.3770773]\n",
      "step = 6300 cost = 0.014785193 W = [[7.724701]\n",
      " [7.727425]] b = [-3.3936117]\n",
      "step = 6400 cost = 0.014551692 W = [[7.7569194]\n",
      " [7.759558 ]] b = [-3.4098825]\n",
      "step = 6500 cost = 0.014325356 W = [[7.788637]\n",
      " [7.791195]] b = [-3.4258993]\n",
      "step = 6600 cost = 0.014105909 W = [[7.81987  ]\n",
      " [7.8223515]] b = [-3.4416683]\n",
      "step = 6700 cost = 0.013892972 W = [[7.8506336]\n",
      " [7.8530407]] b = [-3.4571977]\n",
      "step = 6800 cost = 0.013686268 W = [[7.8809404]\n",
      " [7.883277 ]] b = [-3.4724946]\n",
      "step = 6900 cost = 0.0134856645 W = [[7.9108033]\n",
      " [7.9130726]] b = [-3.4875662]\n",
      "step = 7000 cost = 0.013290717 W = [[7.9402356]\n",
      " [7.9424405]] b = [-3.5024185]\n",
      "step = 7100 cost = 0.013101313 W = [[7.9692497]\n",
      " [7.9713917]] b = [-3.517058]\n",
      "step = 7200 cost = 0.012917127 W = [[7.997857 ]\n",
      " [7.9999394]] b = [-3.5314898]\n",
      "step = 7300 cost = 0.012738065 W = [[8.026067]\n",
      " [8.028094]] b = [-3.5457213]\n",
      "step = 7400 cost = 0.012563785 W = [[8.053893]\n",
      " [8.055866]] b = [-3.5597568]\n",
      "step = 7500 cost = 0.012394178 W = [[8.081345]\n",
      " [8.083267]] b = [-3.5736017]\n",
      "step = 7600 cost = 0.012229072 W = [[8.108432]\n",
      " [8.110303]] b = [-3.5872602]\n",
      "step = 7700 cost = 0.012068221 W = [[8.135167]\n",
      " [8.136984]] b = [-3.6007392]\n",
      "step = 7800 cost = 0.011911576 W = [[8.1615505]\n",
      " [8.163324 ]] b = [-3.6140423]\n",
      "step = 7900 cost = 0.011758905 W = [[8.1876  ]\n",
      " [8.189326]] b = [-3.627174]\n",
      "step = 8000 cost = 0.011609994 W = [[8.213317]\n",
      " [8.215006]] b = [-3.6401381]\n",
      "step = 8100 cost = 0.011464808 W = [[8.238717]\n",
      " [8.24036 ]] b = [-3.6529396]\n",
      "step = 8200 cost = 0.011323165 W = [[8.2638035]\n",
      " [8.265405 ]] b = [-3.6655815]\n",
      "step = 8300 cost = 0.011184953 W = [[8.288581]\n",
      " [8.290146]] b = [-3.6780689]\n",
      "step = 8400 cost = 0.011050081 W = [[8.313062]\n",
      " [8.314591]] b = [-3.6904042]\n",
      "step = 8500 cost = 0.010918393 W = [[8.337252]\n",
      " [8.338745]] b = [-3.7025921]\n",
      "step = 8600 cost = 0.010789707 W = [[8.361158]\n",
      " [8.362616]] b = [-3.714636]\n",
      "step = 8700 cost = 0.010664064 W = [[8.384788]\n",
      " [8.38621 ]] b = [-3.7265391]\n",
      "step = 8800 cost = 0.010541239 W = [[8.408147]\n",
      " [8.409535]] b = [-3.7383041]\n",
      "step = 8900 cost = 0.010421226 W = [[8.431235]\n",
      " [8.432598]] b = [-3.7499347]\n",
      "step = 9000 cost = 0.010303904 W = [[8.454067]\n",
      " [8.455398]] b = [-3.761434]\n",
      "step = 9100 cost = 0.010189181 W = [[8.47665 ]\n",
      " [8.477946]] b = [-3.7728043]\n",
      "step = 9200 cost = 0.010076873 W = [[8.498977]\n",
      " [8.500252]] b = [-3.7840483]\n",
      "step = 9300 cost = 0.00996713 W = [[8.521067]\n",
      " [8.522307]] b = [-3.7951698]\n",
      "step = 9400 cost = 0.0098596625 W = [[8.542914]\n",
      " [8.544133]] b = [-3.8061707]\n",
      "step = 9500 cost = 0.009754456 W = [[8.564536]\n",
      " [8.565722]] b = [-3.8170538]\n",
      "step = 9600 cost = 0.0096514765 W = [[8.585921]\n",
      " [8.587087]] b = [-3.827822]\n",
      "step = 9700 cost = 0.00955065 W = [[8.607088 ]\n",
      " [8.6082325]] b = [-3.8384774]\n",
      "step = 9800 cost = 0.009451836 W = [[8.62804 ]\n",
      " [8.629154]] b = [-3.8490214]\n",
      "step = 9900 cost = 0.009355037 W = [[8.648771]\n",
      " [8.649864]] b = [-3.8594575]\n",
      "step = 10000 cost = 0.009260265 W = [[8.669294]\n",
      " [8.670366]] b = [-3.8697865]\n",
      "\\hypothesis : [[0.02043647]\n",
      " [0.9918421 ]\n",
      " [0.99183345]\n",
      " [0.99999857]] \n",
      "Correct : [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'y-input')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#활성화 함수(hypothesis : 가설)\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# 예측값 결과 계산 및 정밀도 계산\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W =\", sess.run(W), \"b =\", sess.run(b))\n",
    "            \n",
    "    # Accuracy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis :\", h, \"\\nCorrect :\", c, \"\\nAccuracy :\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4️⃣ single neural network XOR 연산 💥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 cost = 1.147689 W = [[0.54303825]\n",
      " [0.22509597]] b = [1.6586293]\n",
      "step = 100 cost = 0.6977366 W = [[-0.13235211]\n",
      " [-0.31579468]] b = [0.31076336]\n",
      "step = 200 cost = 0.69505054 W = [[-0.1127404 ]\n",
      " [-0.21080059]] b = [0.19272244]\n",
      "step = 300 cost = 0.69399035 W = [[-0.08325413]\n",
      " [-0.13564825]] b = [0.12985343]\n",
      "step = 400 cost = 0.69352436 W = [[-0.05990801]\n",
      " [-0.08789951]] b = [0.08766488]\n",
      "step = 500 cost = 0.693317 W = [[-0.04241316]\n",
      " [-0.05736698]] b = [0.05917792]\n",
      "step = 600 cost = 0.69322395 W = [[-0.02968203]\n",
      " [-0.03767065]] b = [0.03994525]\n",
      "step = 700 cost = 0.693182 W = [[-0.02059721]\n",
      " [-0.02486487]] b = [0.02696234]\n",
      "step = 800 cost = 0.693163 W = [[-0.01420295]\n",
      " [-0.01648277]] b = [0.01819881]\n",
      "step = 900 cost = 0.69315445 W = [[-0.00974699]\n",
      " [-0.0109649 ]] b = [0.01228362]\n",
      "step = 1000 cost = 0.69315046 W = [[-0.0066646 ]\n",
      " [-0.00731522]] b = [0.00829102]\n",
      "step = 1100 cost = 0.6931487 W = [[-0.00454415]\n",
      " [-0.00489172]] b = [0.00559614]\n",
      "step = 1200 cost = 0.69314784 W = [[-0.0030916 ]\n",
      " [-0.00327728]] b = [0.00377721]\n",
      "step = 1300 cost = 0.6931475 W = [[-0.00209979]\n",
      " [-0.00219899]] b = [0.00254949]\n",
      "step = 1400 cost = 0.6931473 W = [[-0.00142427]\n",
      " [-0.00147727]] b = [0.00172084]\n",
      "step = 1500 cost = 0.6931473 W = [[-0.00096507]\n",
      " [-0.00099339]] b = [0.00116152]\n",
      "step = 1600 cost = 0.6931472 W = [[-0.0006534 ]\n",
      " [-0.00066852]] b = [0.00078401]\n",
      "step = 1700 cost = 0.6931472 W = [[-0.00044209]\n",
      " [-0.00045017]] b = [0.00052917]\n",
      "step = 1800 cost = 0.6931472 W = [[-0.00029897]\n",
      " [-0.0003033 ]] b = [0.00035724]\n",
      "step = 1900 cost = 0.6931472 W = [[-0.00020211]\n",
      " [-0.00020442]] b = [0.00024111]\n",
      "step = 2000 cost = 0.6931472 W = [[-0.00013658]\n",
      " [-0.00013781]] b = [0.00016274]\n",
      "step = 2100 cost = 0.6931472 W = [[-9.226608e-05]\n",
      " [-9.292497e-05]] b = [0.00010984]\n",
      "step = 2200 cost = 0.6931472 W = [[-6.2319232e-05]\n",
      " [-6.2669635e-05]] b = [7.413744e-05]\n",
      "step = 2300 cost = 0.6931472 W = [[-4.2083451e-05]\n",
      " [-4.2266965e-05]] b = [5.00363e-05]\n",
      "step = 2400 cost = 0.6931472 W = [[-2.8414612e-05]\n",
      " [-2.8508723e-05]] b = [3.377318e-05]\n",
      "step = 2500 cost = 0.6931472 W = [[-1.9182600e-05]\n",
      " [-1.9230512e-05]] b = [2.2796245e-05]\n",
      "step = 2600 cost = 0.6931472 W = [[-1.2946463e-05]\n",
      " [-1.2970533e-05]] b = [1.538739e-05]\n",
      "step = 2700 cost = 0.6931472 W = [[-8.734650e-06]\n",
      " [-8.746798e-06]] b = [1.0387308e-05]\n",
      "step = 2800 cost = 0.6931472 W = [[-5.8952332e-06]\n",
      " [-5.9044014e-06]] b = [7.0039987e-06]\n",
      "step = 2900 cost = 0.6931472 W = [[-3.9699994e-06]\n",
      " [-3.9702277e-06]] b = [4.744983e-06]\n",
      "step = 3000 cost = 0.6931472 W = [[-2.6706193e-06]\n",
      " [-2.6693576e-06]] b = [3.2131416e-06]\n",
      "step = 3100 cost = 0.6931472 W = [[-1.8063497e-06]\n",
      " [-1.8050880e-06]] b = [2.1626088e-06]\n",
      "step = 3200 cost = 0.6931472 W = [[-1.2192434e-06]\n",
      " [-1.2179817e-06]] b = [1.468216e-06]\n",
      "step = 3300 cost = 0.6931472 W = [[-8.3926136e-07]\n",
      " [-8.3799966e-07]] b = [9.571029e-07]\n",
      "step = 3400 cost = 0.6931472 W = [[-5.5390365e-07]\n",
      " [-5.5264195e-07]] b = [6.702566e-07]\n",
      "step = 3500 cost = 0.6931472 W = [[-3.5869797e-07]\n",
      " [-3.5743628e-07]] b = [4.7654268e-07]\n",
      "step = 3600 cost = 0.6931472 W = [[-2.4619447e-07]\n",
      " [-2.4493278e-07]] b = [3.3423674e-07]\n",
      "step = 3700 cost = 0.6931472 W = [[-1.7168853e-07]\n",
      " [-1.7042680e-07]] b = [2.5973208e-07]\n",
      "step = 3800 cost = 0.6931472 W = [[-1.1431884e-07]\n",
      " [-1.1305711e-07]] b = [1.6808998e-07]\n",
      "step = 3900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 4900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 5900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 6900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 7900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 8900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9100 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9200 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9300 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9400 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9500 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9600 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9700 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9800 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 9900 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n",
      "step = 10000 cost = 0.6931472 W = [[-8.8986766e-08]\n",
      " [-8.7725041e-08]] b = [1.1891597e-07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hypothesis : [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct : [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'y-input')\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#활성화 함수(hypothesis : 가설)\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost / loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "# 예측값 결과 계산 및 정밀도 계산\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W =\", sess.run(W), \"b =\", sess.run(b))\n",
    "            \n",
    "    # Accuracy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis :\", h, \"\\nCorrect :\", c, \"\\nAccuracy :\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4️⃣ single neural network XOR 연산  => 해결!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 값 :(0, 0)출력 값 :0\n",
      "입력 값 :(1, 0)출력 값 :1\n",
      "입력 값 :(0, 1)출력 값 :1\n",
      "입력 값 :(1, 1)출력 값 :0\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 바이어스\n",
    "w11 = np.array([-2, -2])\n",
    "w12 = np.array([2, 2])\n",
    "w2 = np.array([1, 1])\n",
    "b1 = 3\n",
    "b2 = -1\n",
    "b3 = -1\n",
    "\n",
    "# 퍼셉트론\n",
    "def MLP(x, w, b):\n",
    "    y = np.sum(w * x) + b\n",
    "    if y <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# NAND 게이트\n",
    "def NAND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w11, b1)\n",
    "\n",
    "# OR 게이트\n",
    "def OR(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w12, b2)\n",
    "\n",
    "# AND 게이트\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w2, b3)\n",
    "\n",
    "# XOR 게이트\n",
    "def XOR(x1, x2):\n",
    "    return AND(NAND(x1, x2), OR(x1, x2))\n",
    "\n",
    "# x1, x2 값을 번갈아 대입해 가며 최종값 출력\n",
    "if __name__ == '__main__':\n",
    "    for x in [(0,0), (1,0), (0,1), (1,1)]:\n",
    "        y = XOR(x[0], x[1])\n",
    "        print(\"입력 값 :\" + str(x) + \"출력 값 :\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 값 :(0, 0)출력 값 :0\n",
      "입력 값 :(1, 0)출력 값 :1\n",
      "입력 값 :(0, 1)출력 값 :1\n",
      "입력 값 :(1, 1)출력 값 :0\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 바이어스\n",
    "w11 = np.array([-7.40, -7.40])\n",
    "w12 = np.array([8.67, 8.67])\n",
    "w2 = np.array([7.41, 7.41])\n",
    "b1 = 11.28\n",
    "b2 = -3.87\n",
    "b3 = -11.29\n",
    "\n",
    "# 퍼셉트론\n",
    "def MLP(x, w, b):\n",
    "    y = np.sum(w * x) + b\n",
    "    if y <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# NAND 게이트\n",
    "def NAND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w11, b1)\n",
    "\n",
    "# OR 게이트\n",
    "def OR(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w12, b2)\n",
    "\n",
    "# AND 게이트\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w2, b3)\n",
    "\n",
    "# XOR 게이트\n",
    "def XOR(x1, x2):\n",
    "    return AND(NAND(x1, x2), OR(x1, x2))\n",
    "\n",
    "# x1, x2 값을 번갈아 대입해 가며 최종값 출력\n",
    "if __name__ == '__main__':\n",
    "    for x in [(0,0), (1,0), (0,1), (1,1)]:\n",
    "        y = XOR(x[0], x[1])\n",
    "        print(\"입력 값 :\" + str(x) + \"출력 값 :\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 cost = 0.7127425 W2 = [[-1.0439962 ]\n",
      " [-0.35504904]]\n",
      "step = 100 cost = 0.6919861 W2 = [[-0.9733313 ]\n",
      " [-0.27967462]]\n",
      "step = 200 cost = 0.69140023 W2 = [[-0.9719322 ]\n",
      " [-0.30612385]]\n",
      "step = 300 cost = 0.69072264 W2 = [[-0.9758814 ]\n",
      " [-0.34338722]]\n",
      "step = 400 cost = 0.6898404 W2 = [[-0.9823105 ]\n",
      " [-0.38849455]]\n",
      "step = 500 cost = 0.68866503 W2 = [[-0.9914159 ]\n",
      " [-0.44299483]]\n",
      "step = 600 cost = 0.6870754 W2 = [[-1.003686  ]\n",
      " [-0.50899225]]\n",
      "step = 700 cost = 0.6849079 W2 = [[-1.0197763 ]\n",
      " [-0.58894217]]\n",
      "step = 800 cost = 0.681951 W2 = [[-1.040517  ]\n",
      " [-0.68554235]]\n",
      "step = 900 cost = 0.6779443 W2 = [[-1.0669235]\n",
      " [-0.8015172]]\n",
      "step = 1000 cost = 0.67258817 W2 = [[-1.1001972 ]\n",
      " [-0.93929684]]\n",
      "step = 1100 cost = 0.66555375 W2 = [[-1.1417398]\n",
      " [-1.1006715]]\n",
      "step = 1200 cost = 0.6564859 W2 = [[-1.1932102]\n",
      " [-1.2865418]]\n",
      "step = 1300 cost = 0.64499897 W2 = [[-1.2566508]\n",
      " [-1.4968084]]\n",
      "step = 1400 cost = 0.63068587 W2 = [[-1.3346751]\n",
      " [-1.7303244]]\n",
      "step = 1500 cost = 0.6131715 W2 = [[-1.4306113]\n",
      " [-1.9848126]]\n",
      "step = 1600 cost = 0.5922049 W2 = [[-1.5484424]\n",
      " [-2.2567928]]\n",
      "step = 1700 cost = 0.56773436 W2 = [[-1.6923742]\n",
      " [-2.5417266]]\n",
      "step = 1800 cost = 0.5399105 W2 = [[-1.8660336]\n",
      " [-2.8345401]]\n",
      "step = 1900 cost = 0.5090352 W2 = [[-2.0715158]\n",
      " [-3.1303196]]\n",
      "step = 2000 cost = 0.47553682 W2 = [[-2.3086033]\n",
      " [-3.4248438]]\n",
      "step = 2100 cost = 0.4400118 W2 = [[-2.5744014]\n",
      " [-3.714758 ]]\n",
      "step = 2200 cost = 0.40328586 W2 = [[-2.863473 ]\n",
      " [-3.9974873]]\n",
      "step = 2300 cost = 0.3664013 W2 = [[-3.1685035]\n",
      " [-4.2710614]]\n",
      "step = 2400 cost = 0.33048907 W2 = [[-3.4813542]\n",
      " [-4.533979 ]]\n",
      "step = 2500 cost = 0.29657137 W2 = [[-3.794229]\n",
      " [-4.785163]]\n",
      "step = 2600 cost = 0.26539743 W2 = [[-4.1005917]\n",
      " [-5.023929 ]]\n",
      "step = 2700 cost = 0.2373738 W2 = [[-4.395633]\n",
      " [-5.249983]]\n",
      "step = 2800 cost = 0.21259509 W2 = [[-4.6763034]\n",
      " [-5.463371 ]]\n",
      "step = 2900 cost = 0.19092818 W2 = [[-4.9410453]\n",
      " [-5.664417 ]]\n",
      "step = 3000 cost = 0.17210585 W2 = [[-5.18942 ]\n",
      " [-5.853656]]\n",
      "step = 3100 cost = 0.15580197 W2 = [[-5.421743 ]\n",
      " [-6.0317507]]\n",
      "step = 3200 cost = 0.14168252 W2 = [[-5.6387835]\n",
      " [-6.1994343]]\n",
      "step = 3300 cost = 0.12943412 W2 = [[-5.841547 ]\n",
      " [-6.3574605]]\n",
      "step = 3400 cost = 0.11877716 W2 = [[-6.0311365]\n",
      " [-6.5065694]]\n",
      "step = 3500 cost = 0.109469846 W2 = [[-6.20866 ]\n",
      " [-6.647471]]\n",
      "step = 3600 cost = 0.10130652 W2 = [[-6.3751826]\n",
      " [-6.7808313]]\n",
      "step = 3700 cost = 0.094114214 W2 = [[-6.5316978]\n",
      " [-6.9072666]]\n",
      "step = 3800 cost = 0.0877487 W2 = [[-6.6791058]\n",
      " [-7.027338 ]]\n",
      "step = 3900 cost = 0.0820892 W2 = [[-6.818233]\n",
      " [-7.14156 ]]\n",
      "step = 4000 cost = 0.07703528 W2 = [[-6.9498177]\n",
      " [-7.250397 ]]\n",
      "step = 4100 cost = 0.07250261 W2 = [[-7.074522 ]\n",
      " [-7.3542767]]\n",
      "step = 4200 cost = 0.06842106 W2 = [[-7.192939]\n",
      " [-7.453567]]\n",
      "step = 4300 cost = 0.06473121 W2 = [[-7.305597]\n",
      " [-7.548618]]\n",
      "step = 4400 cost = 0.061383184 W2 = [[-7.412969]\n",
      " [-7.639743]]\n",
      "step = 4500 cost = 0.058334563 W2 = [[-7.5154843]\n",
      " [-7.727222 ]]\n",
      "step = 4600 cost = 0.055549502 W2 = [[-7.6135116]\n",
      " [-7.811311 ]]\n",
      "step = 4700 cost = 0.05299708 W2 = [[-7.7073965]\n",
      " [-7.89224  ]]\n",
      "step = 4800 cost = 0.05065101 W2 = [[-7.797446 ]\n",
      " [-7.9702225]]\n",
      "step = 4900 cost = 0.048488416 W2 = [[-7.8839355]\n",
      " [-8.045448 ]]\n",
      "step = 5000 cost = 0.04648981 W2 = [[-7.967117]\n",
      " [-8.118092]]\n",
      "step = 5100 cost = 0.044638045 W2 = [[-8.047212]\n",
      " [-8.188314]]\n",
      "step = 5200 cost = 0.04291825 W2 = [[-8.124426]\n",
      " [-8.256263]]\n",
      "step = 5300 cost = 0.041317515 W2 = [[-8.198948]\n",
      " [-8.322069]]\n",
      "step = 5400 cost = 0.039824344 W2 = [[-8.270946]\n",
      " [-8.385859]]\n",
      "step = 5500 cost = 0.038428664 W2 = [[-8.340577]\n",
      " [-8.447744]]\n",
      "step = 5600 cost = 0.037121706 W2 = [[-8.407981]\n",
      " [-8.507831]]\n",
      "step = 5700 cost = 0.035895515 W2 = [[-8.4732895]\n",
      " [-8.566216 ]]\n",
      "step = 5800 cost = 0.03474316 W2 = [[-8.536624]\n",
      " [-8.622985]]\n",
      "step = 5900 cost = 0.033658393 W2 = [[-8.598089]\n",
      " [-8.678225]]\n",
      "step = 6000 cost = 0.032635648 W2 = [[-8.657789]\n",
      " [-8.73201 ]]\n",
      "step = 6100 cost = 0.03166993 W2 = [[-8.715816]\n",
      " [-8.784416]]\n",
      "step = 6200 cost = 0.030756745 W2 = [[-8.772258]\n",
      " [-8.835503]]\n",
      "step = 6300 cost = 0.029892094 W2 = [[-8.827198]\n",
      " [-8.885335]]\n",
      "step = 6400 cost = 0.029072367 W2 = [[-8.880706 ]\n",
      " [-8.9339695]]\n",
      "step = 6500 cost = 0.02829421 W2 = [[-8.932855]\n",
      " [-8.981463]]\n",
      "step = 6600 cost = 0.027554613 W2 = [[-8.983706 ]\n",
      " [-9.0278635]]\n",
      "step = 6700 cost = 0.026850903 W2 = [[-9.033323]\n",
      " [-9.073221]]\n",
      "step = 6800 cost = 0.026180599 W2 = [[-9.081762]\n",
      " [-9.117578]]\n",
      "step = 6900 cost = 0.025541432 W2 = [[-9.129075]\n",
      " [-9.160975]]\n",
      "step = 7000 cost = 0.024931353 W2 = [[-9.175312]\n",
      " [-9.203456]]\n",
      "step = 7100 cost = 0.024348505 W2 = [[-9.220517]\n",
      " [-9.245052]]\n",
      "step = 7200 cost = 0.0237911 W2 = [[-9.264733]\n",
      " [-9.285802]]\n",
      "step = 7300 cost = 0.023257522 W2 = [[-9.308002]\n",
      " [-9.325738]]\n",
      "step = 7400 cost = 0.022746423 W2 = [[-9.350366]\n",
      " [-9.36489 ]]\n",
      "step = 7500 cost = 0.02225641 W2 = [[-9.391856]\n",
      " [-9.403286]]\n",
      "step = 7600 cost = 0.02178614 W2 = [[-9.432506]\n",
      " [-9.440958]]\n",
      "step = 7700 cost = 0.02133461 W2 = [[-9.472351]\n",
      " [-9.477928]]\n",
      "step = 7800 cost = 0.020900674 W2 = [[-9.51142 ]\n",
      " [-9.514225]]\n",
      "step = 7900 cost = 0.020483375 W2 = [[-9.549741]\n",
      " [-9.549871]]\n",
      "step = 8000 cost = 0.020081762 W2 = [[-9.587345]\n",
      " [-9.584886]]\n",
      "step = 8100 cost = 0.019695 W2 = [[-9.624254]\n",
      " [-9.619293]]\n",
      "step = 8200 cost = 0.019322325 W2 = [[-9.660494]\n",
      " [-9.653112]]\n",
      "step = 8300 cost = 0.01896299 W2 = [[-9.696087]\n",
      " [-9.686364]]\n",
      "step = 8400 cost = 0.018616287 W2 = [[-9.731058]\n",
      " [-9.719064]]\n",
      "step = 8500 cost = 0.018281605 W2 = [[-9.765422]\n",
      " [-9.751233]]\n",
      "step = 8600 cost = 0.017958298 W2 = [[-9.799204]\n",
      " [-9.782884]]\n",
      "step = 8700 cost = 0.017645836 W2 = [[-9.832424]\n",
      " [-9.814035]]\n",
      "step = 8800 cost = 0.017343711 W2 = [[-9.865094]\n",
      " [-9.844703]]\n",
      "step = 8900 cost = 0.017051382 W2 = [[-9.897237]\n",
      " [-9.874901]]\n",
      "step = 9000 cost = 0.016768461 W2 = [[-9.928865]\n",
      " [-9.904644]]\n",
      "step = 9100 cost = 0.016494483 W2 = [[-9.959999]\n",
      " [-9.933941]]\n",
      "step = 9200 cost = 0.016228978 W2 = [[-9.99065 ]\n",
      " [-9.962812]]\n",
      "step = 9300 cost = 0.01597158 W2 = [[-10.020835]\n",
      " [ -9.991261]]\n",
      "step = 9400 cost = 0.015721992 W2 = [[-10.050568]\n",
      " [-10.019304]]\n",
      "step = 9500 cost = 0.015479866 W2 = [[-10.079857]\n",
      " [-10.046954]]\n",
      "step = 9600 cost = 0.015244822 W2 = [[-10.108722]\n",
      " [-10.07422 ]]\n",
      "step = 9700 cost = 0.0150166275 W2 = [[-10.137167]\n",
      " [-10.10111 ]]\n",
      "step = 9800 cost = 0.014794883 W2 = [[-10.165209]\n",
      " [-10.12764 ]]\n",
      "step = 9900 cost = 0.014579467 W2 = [[-10.192858]\n",
      " [-10.153813]]\n",
      "step = 10000 cost = 0.014369993 W2 = [[-10.220124]\n",
      " [-10.179639]]\n",
      "\n",
      "Hypothesis : \n",
      " [[0.01183227]\n",
      " [0.9844234 ]\n",
      " [0.98441565]\n",
      " [0.01407099]] \n",
      "Correct : \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy : \n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step =\", step, \"cost =\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W2 =\", sess.run(W2))\n",
    "            \n",
    "    # Accuracy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis : \\n\", h, \"\\nCorrect : \\n\", c, \"\\nAccuracy : \\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
